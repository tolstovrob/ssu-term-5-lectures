#show heading.where(level: 1): it => {
  pagebreak(weak: true)
  it
}

= Численные методы решения СЛАУ

Метод Гаусса решения СЛАУ появился в 1849 году (на момент 2025 целых 176 лет назад).

== Метод Гаусса решения СЛАУ

Рассмотрим СЛАУ следующего вида:

$
  cases(
    a_11 x_1 + a_12 x_2 + ... + a_(1n)x_n = b_1\,,
    a_12 x_1 + a_22 x_2 + ... + a_(2n)x_n = b_2\,,
    ...,
    a_(n 1) x_1 + a_(n 2) x_2 + ... + a_(n n)x_n = b_n\,
  )
$ <raz2:par1:eq1>

или в эквивалентном виде:

$
  mat(a_11, ..., a_(1 n); dots.v, dots.down, dots.v; a_(n 1), ..., a_(n n)) mat(x_1; dots.v; x_n) = mat(b_1; dots.v; b_n),
$ <raz2:par1:eq2>

или в эквивалентном виде через расширенную матрицу коэффициентов (РМК):

$
  mat(
  a_11, dots, a_(1 n), b_1;
  a_21, dots, a_(2 n), b_2;
  dots.v, dots.down, dots.v;
  a_(n 1), dots, a_(n n), b_n; augment: #3)
$ <raz2:par1:eq3>

Далее _будем предполагать_, что СЛАУ *имеет единственное решение*. Метод Гаусса условно разделяют на 2 этапа: прямой и обратный ходы.

Рассмотрим их подробнее.

=== Прямой ход

Сутью прямого хода является эквивалентное преобразование исходной системы к РМК, где все элементы главной диагонли --- единицы, всё что ниже её --- нули, а выше стоят остальные коэффициенты. Так, в самой нижней строке получится:

$
  mat(0, 0, ..., 1, limits(b_n)^tilde; augment: #4)
$

что означает, что в конце СЛАУ будет уравнение вида $x_n = limits(b_n)^tilde$.

Проведём преобразование:

#set math.equation(numbering: none)

$
  & mat(
  a_11, a_12, a_13, dots, a_(1 n), b_1;
  a_21, a_22, a_23, dots, a_(2 n), b_2;
  dots.v, dots.v, dots.v, dots.down, dots.v, dots.v;
  a_(n 1), a_(n 2), a_(n 3), dots, a_(n n), b_n; augment: #5) \
  
  & limits(tilde)^(1 "стр" times 1/a_11, space a_11!=0)

  mat(
  1, a_12^((1)), a_13^((1)), dots, a_(1 n)^((1)), b_1^((1));
  a_21, a_22, a_23, dots, a_(2 n), b_2;
  dots.v, dots.v, dots.v, dots.down, dots.v, dots.v;
  a_(n 1), a_(n 2), a_(n 3), dots, a_(n n), b_n; augment: #5) \

  & limits(tilde)^(2 "стр" - 1 "стр" times a_21)

  mat(
  1, a_12^((1)), a_13^((1)), dots, a_(1 n)^((1)), b_1^((1));
  0, a_22^((1)), a_23^((1)), dots, a_(2 n)^((1)), b_2^((1));
  dots.v, dots.v, dots.v, dots.down, dots.v, dots.v;
  a_(n 1), a_(n 2), a_(n 3), dots, a_(n n), b_n; augment: #5) \

  & limits(tilde)^(3 "стр" - 1 "стр" times a_21) ... \

  & limits(tilde)^(n "стр" - 1 "стр" times a_(n 1))
  
  mat(
  1, a_12^((1)), a_13^((1)), dots, a_(1 n)^((1)), b_1^((1));
  0, a_22^((1)), a_23^((1)), dots, a_(2 n)^((1)), b_2^((1));
  dots.v, dots.v, dots.v, dots.down, dots.v, dots.v;
  0, a_(n 2)^((1)), a_(n 3)^((1)), dots, a_(n n)^((1)), b_n^((1)); augment: #5) \
  
  & limits(tilde)^(2 "стр" times 1/a_22^((1)); a_22 != 0)

  mat(
  1, a_12^((1)), a_13^((1)), dots, a_(1 n)^((1)), b_1^((1));
  0, 1, a_23^((2)), dots, a_(2 n)^((2)), b_2^((2));
  dots.v, dots.v, dots.v, dots.down, dots.v, dots.v;
  0, a_(n 2)^((1)), a_(n 3)^((1)), dots, a_(n n)^((1)), b_n^((1)); augment: #5) \

  & limits(tilde)^(3 "стр" - 2 "стр" times a_31) ... \
  
  & limits(tilde)^(n "стр" - 1 "стр" times a_(n 1)) 
  
  mat(
  1, a_12^((1)), a_13^((1)), dots, a_(1 n)^((1)), b_1^((1));
  0, 1, a_23^((2)), dots, a_(2 n)^((2)), b_2^((2));
  dots.v, dots.v, dots.v, dots.down, dots.v, dots.v;
  0, 0, a_(n 3)^((2)), dots, a_(n n)^((2)), b_n^((2)); augment: #5) \
  
  & limits(tilde)^(3 "стр" times 1/a_33^((1)); a_33 != 0) ... \
  
  & limits(tilde)^(n "стр" times 1/a_(n 1)^((1)); a_(n 2) != 0)

  mat(
  1, a_12^((1)), a_13^((1)), dots, a_(1 n)^((1)), b_1^((1));
  0, 1, a_23^((2)), dots, a_(2 n)^((2)), b_2^((2));
  dots.v, dots.v, dots.v, dots.down, dots.v, dots.v;
  0, 0, 0, dots, a_(n n)^((n)), b_n^((n)); augment: #5).
$

// FIX: В выражении ошибки, надо исправить

Здесь $k^((n))$ --- это не производная $n$-го порядка, а $k$ после $n$-го преобразования.

Продолжая аналогичные операции, а именно выполняя нормировку следующего диагонального элемента, и обнуляя элементы, стоящие ниже него (построчно), за конечное число шагов придём к РМК следующего вида:

#set math.equation(numbering: "(1)")

$
  mat(
  1, a_12^((1)), a_13^((1)), a_14^((1)), dots, a_(1, n-1)^((1)), a_(1 n)^((1)), b_1^((1));
  0, 1, a_23^((2)), a_24^((2)), dots, a_(2, n-1)^((2)), a_(2 n)^((2)), b_2^((2));
  dots.v, dots.v, dots.v, dots.v, dots.down, dots.v, dots.v, dots.v;
  0, 0, 0, 0, dots, a_(n-1, n)^((n)), a_(n n)^((n)), b_n^((n)); augment: #7) .
$ <raz2:par1:eq4>

=== Обратный ход

Восстанавливаем исходную СЛАУ, которая будет эквивалентна исходой по построению новой РМК:

$
  cases(
    x_11 + & a_12^((1)) x_2 + ... + & a_(1 n)^((1)) = b_1^((1))\,,
    & a_12^((1)) x_2 + ... + & a_(1 n)^((1)) = b_1^((1))\,,
    &...,
    & & a_(n n)^((2)) = b_n^((n)).
  )
$

== Процедура выбора главного (ненулевого) элемента

В предыдущем параграфе мы рассмотрели базовую конструкцию метода Гаусса, в рамках которой в прямом ходе мы попутно делали предположения, что:

+ $a_11 != 0$;
+ $a_22^((2)) != 0$;
+ $a_33^((3)) != 0$; 
+ $a_(n n)^((n - 1)) != 0$.

Если на $k$-ом шаге прямого хода мы встретим $a_(k k)^((k - 1)) eq.triple 0$, или в представлении компьютера $a_(k k)^((k - 1)) in O_delta = (-delta, delta)$

В случаях $(k + 1), ..., n space space a_(m k)^((k - 1)) = limits(max)_(k + 1 lt.eq.slant j lt.eq.slant n) |a_(j k)|$.

+ Если $a_(m k)^((k + 1)) != 0 space ("а в компьютере" in.not (-delta, delta))$, то меняют $k$ и $m$ строки РМК.

Тогда, если в позиции $(k, k)$ $a_(m k)^((k - 1)) != 0$, то меняем *строки* и прямой ход по методу Гаусса может продолжаться.

+ В случае $a_(m k)^((k + 1)) = 0$ ищут элемент $a_(k m)^((k - 1)) = limits(max)_(k + 1 lt.eq.slant j lt.eq.slant n) |a_(k j)|$  

Тогда, если в позиции $(k, m)$ $a_(k m)^(k - 1) != 0$, то меняем местами *столбцы* $k$ и $m$.

В ином случае $(a_(k m)^((k - 1)) = 0)$ прямой ход будет остановлен ввиду невозможности нормировки диагонального элемента.

В случае перестановки столбцов в РМК параллельно необходимо запоминать соответствующую перестановку в векторе $x$, с тем чтобы после завершения уже обратного хода выполнить обратную перестановку этих неизвестных, вернув им натуральный порядок нумерации.

=== Нахождение определителя квадратной матрицы *$A_(n times n)$*.

Для матрицы $A = mat(a_11, a_12; a_21, a_22)$ определитель равен $det A = a_11 a_22 - a_12 a_22$.

С ростом размерности матрицы $A$ число операций для нахождения определителя этой матрицы растёт факториально. В этой связи применим к матрице $A$ процедуру ортогонализации.

Применим эквивалентные преобразования:

+ строка $times 1 / a_(k k)^((k - 1))$;
+ строка + строка $times lambda (!= 0)$;
+ меняются местами строки/столбцы в ПВГЭ

Тогда в отношении определителя:

+ $Delta_m = Delta_(m - 1) times 1 / (a_(k k)^(k - 1)) $;
+ $Delta_m = Delta_(m - 1)$;
+ $Delta_m = - Delta_(m - 1)$

Получаем формулу:

$
  Delta = (-1) times a_11 times a_22^((1)) times ... times a_(n n)^((n - 1)).
$

Отметим, что для того, чтобы воспользоваться формулой, нужно провести предварительную диагонализацию матрицы.

== Метод прогонки

(TODO)

== Метод простой итерации решения СЛАУ

По прежнему решаем СЛАУ общего вида:

$ A x = b $ <raz2:par4:eq1>

Предполагаем, что у данной системы существует единственное решение.

Покажем, что СЛАУ вида (@raz2:par4:eq1) преобразуется к следующему виду:

$ x = alpha x + beta $ <raz2:par4:eq1a>

Одним из способов преобразования может быть следующий:

В каждом уравнении системы (@raz2:par4:eq1) в левой части равенства оставляем ту компоненту вектора $x$, что имеет номер текущего уравнения, а именно --- каждое уравнение записываем в виде:

$ x_i = 1/a_(i i) (b_i - limits(sum)_(j = 1 \ j!=i)^n a_(i j) x_j), space i=overline(1\,n). $ <raz2:par4:eq2>

Перепишем систему (@raz2:par4:eq2) в матричном виде:

$
  mat(x_1; x_2; dots.v; x_n) & = mat(0, -a_12 / a_11, -a_13 / a_11, dots, -a_(1 n) / a_11; -a_21 / a_22, 0, -a_23 / a_22, dots, -a_(2 n) / a_22;dots,dots,dots, dots, dots; -a_(n 1) / a_(n n), -a_(n 2) / a_(n n), -a_(n 3) / a_(n n), dots, 0) mat(x_1; x_2; dots.v; x_n) + mat(b_1; b_2; dots.v; b_n) \ & = alpha mat(x_1; x_2; dots.v; x_n) + beta.
$ <raz2:par4:eq2a>

Таким образом, развёрнутая запись системы в виде (@raz2:par4:eq2a) показывает, что СЛАУ (@raz2:par4:eq1) может быть преобразовано к эквивалентному виду (@raz2:par4:eq1a). Другими словами, решить СЛАУ (@raz2:par4:eq1) --- это тоже самое, что решить СЛАУ (@raz2:par4:eq2).

Метод простой итерации основан (МПИ) основан на итерационном (рекурсивном, повторяющемся) построении последовательности векторов

$ {x^((k))}_(k = 0) ^ oo : x^((k + 1)) = alpha x^((k)) + beta, $ <raz2:par4:eq3>

где некоторым образом задан $x^((0))$. Формулу (@raz2:par4:eq3) называют _формулой МПИ_.

#figure(
  image("images/2025-10-28.png"),
  supplement: none,
)

Выясним, при каких условиях последовательность векторов (@raz2:par4:eq3). будет сходиться к точному решению СЛАУ (@raz2:par4:eq2). При этом значение $x^((0))$ будем считать заданным, но пока неизвестным для нас образом.

Так как $x^*$ есть точное решение СЛАУ (@raz2:par4:eq2), то имеет место следующее тождество:

$ x^* = alpha x^* + beta $ <raz2:par4:eq4>

Составим разность равенств (@raz2:par4:eq4) и (@raz2:par4:eq3) при некотором фиксированным $k$:

$ x^* - x^((k + 1)) = alpha (x^* - x^((k))) <=> epsilon^((k + 1)) = alpha epsilon^((k)) space forall k = overline(0\,oo) $ <raz2:par4:eq5>

Используя представление (@raz2:par4:eq5), получаем:

$
  cases(
    & e^((1)) = alpha e^((0)), \
    & e^((2)) = alpha e^((1)) = alpha alpha e^((0)) = alpha^2 e^((0)),\
    & e^((3)) = alpha e^((2)) = alpha alpha e^((1)) = alpha alpha alpha e^((0)) = alpha^3 e^((0)), \
    & ... \
    & e^((k + 1)) = ... = alpha^(k + 1) epsilon^((0)).
  )
$ <raz2:par4:eq6>

Таким образом имеем последовательность векторов-погрешностей ${epsilon^((k))}_(k = 0)^oo$, определяемых по формулам (@raz1:par1:eq6) или (@raz1:par1:eq5).

Согласно «известному факту», для того чтобы последовательность векторов сходилась к предельному вектору, необходимо чтобы последовательность норм этих векторов сходилась к соответствующей норме предельного вектора. То есть:

$ {x^((k))}_(k = 0)^oo -> x^((n)) <=> {||x^((k))||}_(k = 0)^oo -> ||x^((n))||. $

Перейдём к соответствующему равенству норм:

$ & ||e^((k + 1))|| \ & = ||limits(underbrace(alpha^(k + 1)))_(in space RR^(n times n) \ ||dot||_(RR^(n times n))) limits(underbrace(epsilon^((0))))_(in space RR^(n) \ ||dot||_(RR^(n)))|| \ & lt.eq.slant ||alpha^(k + 1)||_(RR^(n times n)) dot ||epsilon^(0)||_(RR^(n)) \ & = ||alpha||^(k + 1)_(RR^(n times n)) dot ||epsilon^(0)||_(RR^(n)) $

Из последней цепочки равенств/неравенств получаем равентво:
$
  limits(underbrace(||epsilon^((k + 1))||))_(in RR) = limits(underbrace(||alpha||^((k + 1))))_(in RR) dot limits(underbrace(||epsilon^((0))||))_(in RR)
  quad forall k = overline(0\, infinity),
$ <raz1:par1:eq7>

которое выполняется при условии согласованности норм.

Итак:

$ {||epsilon^(k + 1)||}_(k = 0)^oo -> 0_(RR), space ||alpha|| < 1. $ <raz1:par1:eq8>

Другими словами:

$ {epsilon^(k + 1)}_(k = 0)^oo -> 0_(RR^n) $

И далее:

$ {x^* - x^((k + 1))} -> 0_(RR^n) <=> {x^((k + 1))} -> x^* <=> {x^((k))} -> x^*. $  <raz1:par1:eq9>

Таким образом, из вышеизложенного, получаем:

/ Теорема.: Чтобы последовательность векторов ${X^((k))}$, вычисляемая по итерационной формуле (@raz1:par1:eq3), сходилась к точному решению СЛАУ (@raz1:par1:eq2) --- $x^*$, необходимо:

1. Наличие согласованности норм $||dot||_(RR^n)$ и $||dot||_R^(n times n)$,
2. Выполняемость условия сходимости МПИ (@raz1:par1:eq8) ($||alpha|| < 1$).

Пример согласованных норм:

$ ||x||_(RR^n) = limits(max)_(1 lt.eq.slant i lt.eq.slant n) |x_i|, space x = mat(x_1;x_2;dots.v;x_n) in RR^n, $ и $ ||alpha||_(RR^(n times n)) = limits(max)_(1 lt.eq.slant i lt.eq.slant n) limits(sum)_(j = 1)^n |x_i|, space alpha = mat(alpha_11, dots, alpha_(1 n);dots.v,dots.down, dots.v;a_(n 1), dots, a_(n n)) in RR^n. $

// TODO три замечания + формула 10